# C Chip-8

내 프로젝트 중 C언어로 작성된 첫번째 프로젝트인 Chip-8 에뮬레이터.

최대한 외부 자료를 참고하지 않고, 직접 구현해볼 예정.   
하더라도 chip-8 코드가 아니라 특정 기능 구현에 참고할만한 코드를 찾아볼 것.

프로젝트가 진행되면서 충분히 내용이 길어지기 전까진 README에 의사결정 과정이나 참고 자료를 작성함.

# 구현 기록

## 1. 타이머 구현하기

일단 여러가지 요소가 있지만, 중요하면서 구현방법이 생각나지 않는 타이머를 먼저 구현하기로 함.

### 1.1 뭘 찾아야 할까?

예전에 PUBG의 개발 블로그에서 FPS 관련된 내용을 찾아봤던 기억이 남.

- https://blog.naver.com/whitesky9618/221476284242 - 원본을 찾을 수 없음
- 서버에서 프레임 단위로 연산을 수행한다는 사실을 알았고, 그게 연산 Cycle과 비슷하다는 느낌을 받음.

근데 더 찾아보니까 약간 결이 다른듯

- https://youtu.be/YHswt4VCeJs?si=-CY9b1UaGzAbje4f
- https://developer.valvesoftware.com/wiki/Latency_Compensating_Methods_in_Client/Server_In-game_Protocol_Design_and_Optimization

많은 자료가 클라이언트/서버 간의 불일치를 해결하고 보정하는데, 중점이 맞춰있는듯  
나는 프레임을 어떻게 구현하는지가 궁금한건데 그건 많이 없음

게임 서버가 아니라, 클라이언트 쪽이 더 애뮬레이터 개발과 비슷한 것 같지만,   
다들 Unity를 사용하지, 직접 구현하는 쪽은 없는 것 같아서 다른 식으로 찾아봐야 할 듯.

### 1.2 Fixed Timestep, Variable Delta Time

더 조사해본 결과, 게임에서 연산을 수행하는 사이클을 크게 2가지 방식으로 구현하는 것 같음

- 용어 정리
    - Δt(델타 티, delta time): 시간 간격, 두 시점 사이에 경과한 시간
        - 캐릭터 움직임, 물리 시뮬레이션 등 모든 시간 기반 계산에 사용됨
        - 보통 프레임 간의 시간 차이로 구할 수 있음

1. Fixed Timestep (고정 Δt)
    - 매 업데이트마다 동일한 Δt(예: 1/60초)로 물리 시뮬레이션을 수행
    - 결정론(determinism)이 보장되어 네트워크 락스텝, 리플레이 기능 구현에 유리
    - 렌더링 프레임 레이트와 독립적이므로, 낮은 성능 환경에서 잔상(stuttering)이 발생하거나
      과도한 계산 부하가 생길 수 있음
2. Variable Delta Time (가변 Δt)
    - 매 프레임 실제 경과 시간(hires timer)을 Δt로 측정해 업데이트
    - 프레임 드롭 시 Δt가 커져 물리 속도가 보정되므로 속도 변화가 덜 눈에 띔
    - 그러나 Δt 변화에 민감하여 물리 안정성(충돌 터널링, 발산 등)이 저하되고,
      프레임 레이트에 따라 게임 플레이 “감”이 달라질 수 있음
3. Semi‑Fixed Timestep & Accumulator 패턴
    - Variable Δt의 불안정 문제를 해결하기 위해 최대 Δt를 정해두고,
      남은 프레임 시간을 여러 개의 고정 Δt 스텝으로 분할 처리
    - accumulator에 남는 시간을 보존해 다음 프레임에 반영하므로
      시뮬레이션 속도는 일정하게 유지하면서 렌더링 변동성을 흡수
    - (옵션) 잔여 시간에 대해 이전 상태와 현재 상태를 선형/구면 보간(interpolation)하여
      렌더링하면 부드러운 움직임을 얻을 수 있음

근데? Accumulator 패턴은 이해가 잘 못함.

그리고 간단하게 구현하는데, 낮은 성능 환경을 생각하면서 개발하기는 좀 과하다고 생각함.  
Fixed Timestep를 사용해서 구현하기로 함.

- 참고 자료
    - https://www.reddit.com/r/gamedev/comments/uxvlg7/implementing_ticks_properly/
        - https://gafferongames.com/post/fix_your_timestep/
        - https://gamedev.stackexchange.com/questions/80400/creating-a-tickrate-class
            - game-tick이 Fixed Timestep과 대응되는 표현인 듯

### 1.3 구현방법 생각해보기

https://gafferongames.com/post/fix_your_timestep/ 를 참고하여 구현

```c
double t  = 0.0;            // 시뮬레이션용 내부 시간
double dt = 1.0 / 60.0;     // 한 스텝에 고정할 Δt (1/60초)

while ( !quit )
{
    something(&cpu)              // 에뮬레이터 동작 수행
    t += dt;                     // 내부 시간을 dt만큼 증가
    //TODO: 다음 틱 시각 계산
    //TODO: 남은 시간만큼 sleep, 이미 다음 틱의 시간을 초과한 경우는 고려하지 않음.
}
```

성능 나쁜건 고려 안하고, 남은 시간동안 대기.

이러면 컨텍스트 스위칭이나 Sleep에서 깨는 시간 등으로 변동(jitter)이 생길거 같은데,
남은 시간이 N%(대충 10%?) 정도면 sleep에서 깨워서 busy‑wait 하는 방식도 괜찮을거 같음.

지금은 간단하게 sleep하는 식으로 갈듯.

(추가) 더 일찍 깨어나는 경우는 없고, 1~3ms 정도 일리기만 하는 경우가 많아서 1ms 먼저 깨우고, 
busy‑wait하게 수정해봤더니 Drift가 0~2ms로 줄어들었음. 효과가 있긴 하네
3ms로 늘려서 해보면 수행 시간(더미 코드로 rand sleep)이 0인 경우를 제외하면 0.00ms 보장됨
이건 테스트해본거라 main 코드에 당장 반영하지는 않음

이럼 이제 시간 처리하는 걸 어떻게 구현할 지 찾아봐야 함.

### 1.4 시간 처리 방법 찾기

내가 알기로는 컴퓨터의 시간이 두 종류가 있는데,

- 단조 시간(CLOCK_MONOTONIC)
- NTP 시간(CLOCK_REALTIME)

(DDIA 책에서 봄)

근데 Chip-8에서는 단조 시간을 사용해서 쓰는게 맞음.   
짦은 시간에서 정확해야하고, 남들과 공유하는 시간이 아니니까

그럼 time.h 중 어떤 기능을 사용해야 할까?

https://en.cppreference.com/w/c/chrono/clock 여기에서  
`clock_gettime(CLOCK_MONOTONIC, &s_timespec);`을 사용하면 될 듯?  
CLOCK_MONOTONIC이 단조 시간을 나타내는 ID임.

GPT 사용해서 [test_tick_cycle](./test_tick_cycle.c) 코드를 작성함.
이거 맞는지 체크하고, 직접 구현해보면 될 듯?

다음 틱 예약(next_tick) 누적으로 내부 시뮬레이션 시간은
drift(누적된 시간 차이) 없이 dt만큼 정확히 증가하고.      
매 프레임 ±3ms 정도 jitter는 일반적인 유저레벨 환경에서 기대할 수 있는 정상 범위라고 함.   
이 근거를 좀 찾아봤지만, OS, 유저레벨,  jitter 값이 어느정도인지는 정리된 표가 없음.

일단 간단한 게임에서 이정도 차이는 괜찮아보임.
경쟁 게임인 발로란트에서도 높은 수준의 게임에서 20ms 차이가 승패에 영향을 준다고 했는데, 3ms정도면 뭐

- https://technology.riotgames.com/news/peeking-valorants-netcode

근데 보면서 깨달은게, 로깅이 필요하긴 할 듯? Debug or Trace 에서만 활성화되는 걸로

### 1.5 내가 잘 모르는 용어 정리

지터(Jitter)란?

- 지터는 주기적인 신호나 작업의 타이밍에서 발생하는 불규칙한 변동을 의미합니다.
- 컴퓨팅 시스템에서는 정확한 시간 간격으로 실행되어야 하는 작업이 예상 시간보다 일찍/늦게 실행되는 현상

- 프로그램이 정확히 매 16.67ms(60Hz)마다 작업을 수행하도록 설계되었다면:
    - 지터가 없는 이상적인 상황: 항상 정확히 16.67ms 간격으로 실행
- 실제 상황(지터 있음): 16.5ms, 16.9ms, 16.3ms, 16.8ms 등 약간의 변동이 발생


- RTOS는 GPOS 대비 더 낮은 지터를 가지고 있음.

델타 타임

- DT는 델타 타임(Delta Time)을 의미합니다. 
- 시뮬레이션/게임 루프에서 한 프레임(단계)이 실제 시간으로 얼마나 진행되는지를 나타내는 고정된 시간 간격
- https://en.wikipedia.org/wiki/Delta_timing

### 1.6 구현 고민 1: 시간을 꼭 double로 표현해야 할까?

```c
_STRUCT_TIMESPEC
{
__darwin_time_t tv_sec;
long            tv_nsec;
};
```

부동소수점으로 써도 괜찮은게 맞나? 근데 double 유효자리수가 15정도고, 나노초가 1e9자리니까 괜찮음

- 밀리초(ms)
  - 1밀리초 = 1/1,000초 = 0.001초
  - 사람의 반응 속도나 오디오 지연(latency), 간단한 센서 응답 시간 측정에 자주 사용됩니다.
- 마이크로초(μs)
  - 1마이크로초 = 1/1,000,000초 = 0.000001초
  - 고속 통신, 초고속 센서, 디지털 회로의 스위칭 속도 등을 다룰 때 쓰입니다.
- 나노초(ns)
  - 1나노초 = 1/1,000,000,000초 = 0.000000001초
  - CPU 클럭 주기(예: 3GHz ≒ 0.333ns), 메모리 접근 시간, 초고속 네트워크 패킷 레이턴시 측정 등에 사용됩니다.

## 2. 로깅

### 2.1 구상하기

- 일단 간단하게 매크로나 라이브러리 구현 참고해서 쓸 듯?
- 다만 코드 동작에 영향을 안주게 하려면 별도 스레드 풀에 위임해야 할 듯.
- chip-8은 싱글 스레드고, 로깅을 많이 하면 좀 번거로울수도 있으니까?

적용할만한거
1. 컴파일 타임 로깅 레벨 분리 (매크로를 사용한 컴파일 시에 코드 제거)
2. 별도 쓰레드에서 buffer 만들어두고 비동기 + bulk write로 성능 감소 최소화하기
   - 대용량 버퍼링 & 백그라운드 처리 -> sys‑call 병목 방지
3. 로깅 전략별로 하나하나 켜고 키게 하기?
    - 리눅스 커널 로그 볼 때 ftrace 가 이런 식으로 처리되었던거 같은데
    - 로깅 레벨보다 더 세분화 하는게 괜찮을거 같기도 함.
    - 근데 이정도 프로젝트면 걍 주석 걸거나 그떄그떄 코드 바꿔도 괜찮지 않을까?

- 추가) lock‑free ring buffer 로 여러 스레드에서 효율 높게 쓰는게 가능한듯?
  - io_uring 가 이 기술을 쓰는거같은데, 이 프로젝트는 싱글스레드라 필요 없음

### 2.2 구현하기

초반에는 간단하게 그냥 라이브러리 코드 가져다 쓸듯?

유지보수가 안되는거 같긴 하지만, 약간 불편한게 문제고 못쓸 정도는 아닌듯?

## 3. 에러 처리

이건 별 고민 없이 들었던 강의나 자료에서 쓰는 방식대로 함.

1. 모든 에러는 전역으로 관리 - Enum이 int라 뭔가 꼬일 수도 있어서
2. 에러가 발생할 수 있는 함수는 에러 코드를 명시적으로 반환
3. 프로젝트 자체가 실패해야 하는 에러 코드를 담는 경우, 전역 변수 사용
4. (옵션) 지금은 어떨지 모르겠는데, 나중에 전역 실패가 많아지면 goto로 handler 만들 생각
   - 아마 cpu 에뮬레이터 구현 들어가면서 힙 할당 생기면 바로 추가될거 같긴 하지만

그리고 errno처럼 전역으로 에러 관리하는건 안좋다고는 했는데, 
에러처리 전파가 발생하지 않아 편하고, 처리하는걸 잊어버러기 어려운 정보이므로 괜찮다고 생각함.

https://github.com/rxi/log.c

## 3. 중간 점검

지금 설정된 cpu cylce이 너무 낮음. 몇 백이나 천 단위는 되어야 할 듯?
1MHz면 1_000_000이고, 대충 초당 700~900 번 정도 cpu 명령어 연산이 수행된다고 함.
> 초기 CHIP-8 컴퓨터의 프로세서는 약 1MHz로 작동했고,
> 90년대 HP48 계산기는 약 4MHz로 작동했습니다.

구현이 좀 명확하지 않음. 막 전역변수로 관리되고, main에 뭉쳐있고 하니까 분리하는게 좋을 듯.       
Chip-8 구조체를 만들어서 관리해야 main에 붙이기가 수월할거고...   
구조체 안에 메모리, 레지스터, 등등을 포함시키는걸로?

타이머 구현중인 상태라서 타이머 구현 끝나고 변경함.

## 4. 타이머

### 4.1 구상

- 구현해본적이 없어서 감이 안잡힘
- GPT와 구글의 도움을 받아서 구현 방법을 정리함
  1. 누산기(Accumulator) 
  2. 타이머 인터럽트 
  3. 별도 스레드 
  4. 절대 시간 기반 (CLOCK_MONOTONIC)
- 절대 시간 기반이 현재 cycle 도는 방식임.
- 타이머 인터럽트나 별도 스레드는 정확할 수는 있어도, 구현 난이도가 있어보여서 제외함.
- 절대 시간 시반과 누산기 중 어떤게 나을까?
  - 절대 시간으로 만들었으니까 이번엔 누산기로 쓸 듯

### 4.2 구현

간단하게 누산기 방식으로 구현함.

이 상태를 외부에서 가져다 쓸 수 있게 하는견 별개의 문제지만, 로직 자체는 어렵지 않음.

### 4.3 메모

main의 cycle은 절대 시간 기반 방식. 
즉, 고정 타임스텝 방식(Fixed timestep)을 사용했기 때문에 오차가 누적되지 않는다.  
timespec 이 정수 기반인 것도 있음.  
(근데 생각해보면 t_sim이 double이라 오차 생길 수 있어서, tick 횟수로 관리하는게 나은듯? 
비교도 timespec끼리 하고)   

그러나 타이머의 경우 누산기 방식이라 오차 관리가 필요하다.  
따라서 double을 사용하지 않고 uint64를 사용함.

그냥 부동소수점 수에 1 더하는 식으로만 해도 알 수 있는 사실이니까.  
테스트 코드는 필요 없을 듯?

## 5. 구조 변경

지금 뭔가 이대로는 붙이기 어려울 것 같다는 느낌을 받아서.

리팩토링하기로 함.

### 5.1. 기존 구조 변경하기

1. 시간 계산 개선
    - 부동소수점인 `double` 대신 `uint64_t`를 사용하게 변경 
    - 이 과정에서 년 단위 시간 정보가 날아기는데, 
    chip8에선 실제 시간을 보여줄 필요가 없으므로 상관없음  
      - (2^64를 ns로 하면 약 585년 정도만 표현 가능)
2. 전체적인 코드 개선
   - `struct timespec` 기반의 계산을 개선하면서 코드 양이 줄었음.
   - 시스템 콜 기반의 sleep 대기 대신 busy-wait을 사용하여 빠르게 처리.
     - 이렇게 하지 않으면 시스템 콜 떄문에 2~4ms 딜레이가 생기기도 함.
     - 근데 실제 사이클이 1~2ms 정도여아하니까 너무 느림.
     - 이게 macos라 그런건지 아니면 더 좋은 다른 방법이 있는건지는 모르겠음.

- 개선을 위해서 다른 코드들을 조금 봄
  - 근데 코드가 생각보다 명세를 완전히 지키는 것 같지 않음.
    - 특히 시간 관리하는 부분이 연산 처리하고 N ns 대기 이런 식
    - 1/60s 마다 카운트 처리 같은것도 잘 안되어있고
  - 그리고 화면에 그리는 것 때문에 SDL을 사용하는 것 같음.
    - 나는 CLI로 그리는 걸로 해볼 생각 중.

### 5.2. 구상

Chip-8 구조체를 만들고, 여러 파일에서 가져다 쓰는 식으로 만들어야겠음.

- Chip-8 구성요소 분석
  - 메모리
  - 범용 레지스터
  - 스택 포인터, PC, 메모리 포인터
  - 스택
  - 키보드
  - 디스플레이
  - 타이머, 사운드
  - Instructions(명령어)

### 5.3. chip8 구조체 만들기

참고자료 참고해서 구현 - GPT도 조금 씀. 검증은 했으니 ㄱㅊ

디스플레이가 1비트만 있으면 되는데
이걸 `uint8_t[64*32]`로 할지 `uint64_t[32]`로 할지 고민중

그냥 `uint64_t[32]`로 함. 비트연산 해보고 싶어서

## 6. 타이머 기능 구현하기

구상: 누산기로 구현해서, 사이클마다 시간 체크해서 지났으면 count 값 제거하는 식으로 구현하면 될 듯?

구현: 기존에 구현했던거 재사용해서 씀. 레지스터 값은 chip8, 누산기 값은 static 변수 사용하도록 수정

sound_timer에서 소리 나게하는건 어케하는지 모르겠어서 스킵

## 7. Fetch/Decode/Execute Loop 구현

핵심이 되는 부분, 시간 맞춰서 Loop하는 기능은 구현해둔 상태임.

자료가 잘 나와있어서 구현 자체는 어렵지 않았음. 

SHR, SUBN 같은 용어를 처음봐서 좀 해맸고. 생각보다 난이도가 쉽다?
여기서 이게 입출력이랑 소리, CLI(CLI기반 GUI? 뭐 암튼)만 만들면 되는 것 같음.

nand2tetris의 vm 구현하는거랑 비슷한 느낌.

생각보다 에러 던질만한 부분이 없네, 걍 바로 assert로 실패시키는게 나았으려나?

## 8. ROM 로드 기능 구현 & 기존 chip8 코드 init 로직 개선

파일 IO 부분은 잘 몰라서 GPT & 구글링 도움을 받음. 나중에 공부하면서 개선할 예정.

chip8 값 초기화 부분도 memset으로 한 번에 초기화하도록 개선함.

## 9. 간단한 IBM 로고 그리기 프로그램 로딩&실행 테스트

IBM 로고를 출력하는 간단한 ROM을 github에서 구해오고 로딩하게 설정, 실행해보았다.

지금 구현에선 버그가 있어서 올바른 결과가 나오지 않고 실패 중. 

코드 수정 중...

#### 코드 수정 1 - opcode가 2번째 수행부터 0임.
ROM에 있는 2번째 명령어가 문제인거 같은데, 메모리 뷰 확인해서 어떤 명령어 쓰는지 구함.

가장 앞 4가지

`00 e0 a2 2a   60 0c 61 08`

근데 딱히 문제 될 부분이 없음. 

근데 명령어 로딩 자체는 문제가 없네? 
opcode에 따라 로직 수행하는 거 대신 순차적으로 opcode 읽기만 하게 했더니 잘 읽음.

이후 로직 어딘가에서 opcode가 0으로 설정되어린다는 걸 확인.
디버깅으로 로직 따라가기.
2중 switch문에서 바깓 switch문에 break 사용을 까먹어서 그랬음. 수정 후 opcode가 0인 문제는 해결

#### 코드 수정 2 - vf 레지스터 처리

이거 16번째 레지스터인 vf를 `v[0xf-1]`로 접근하는데 이러면 14임.   
`v[0xf]`가 15로 16번째 레지스터에 접근하는 인덱스가 맞음. 

이거 뭔 정신으로 이렇게 짰는지 모르겠는데, 처음에 짜두고 의심 없이 복붙하니까 그런 듯?
display 쪽 코드 수정하다가 이상한거 알아채서 먼저 수정함.

#### 코드 수정 3 - display 수정

display 쪽이 전체적으로 코드가 좀 이상했음. 

비트 단위로 처리하기 어려워서 `uint8_t[64*32]`로 사용했는데, 
메모리 상에서는 비트 단위로 저장한 걸 쓰려고 하니까.

그래서 해당 부분 코드를 갈아엎기로 함. 
근데 이 부분은 내가 약하기도 하고, 잘 못하는 부분이라 AI 도움을 받음.

일단 display 배열 크기를 보정함. `uint8_t display[32][8];`로 `64 * 32`를 표현함.   
볼때마다 `arr[y][x]` 순으로 쓰는건 햇갈림. 저게 메모리 구조를 생각했을 떄 맞다는걸 알아도 그럼.  
(y가 커질때 큰 폭으로 넘어가고, x가 커지면 y값에 따라 넘어가는 영역 사이의 한 칸마다 움직임)  

심지어 GPT랑 이야기하다가 알게 된게, 저기서 y를 행(row)라고 부르더라, x는 열(column)
나는 엑셀이나 db같은 표에 익숙해서 row는 가로로 긴 데이터, y는 데이터 타입 하나 or 세로로 긴 형식
이런 식으로 생각했는데, 이걸 정리할 필요가 있어보임.

https://chatgpt.com/share/681f5d23-4608-8008-a756-5d3048f4a71a 

일단 대충 구현은 했는데, 그림이 이상하게 그려짐. 

아마 다른 로직에 문제가 있는듯?
각 전체 명령어 로직 테스트를 해봐야 할 듯.

전체 코드 다듬음.

아래는 수정한 버그와 설명
1. opcode 추출 로직 연산자 우선순위로 인한 버그 수정
   - 예: `(opcode & 0x0F00 >> 8)` -> `(opcode & 0x0F00) >> 8`
2. `2nnn`, `7xkk` 버그 수정
   - 요구사항을 잘못 반영한거라 딱히 설명 없음
3. `0x8000`대 명령어 처리 로직에서 switch에 break문 빠진거 추가
4. `Dxyn`의 display 출력 버그 수정
   - 작성하려는 스프라이트 픽셀의 값이 어떻든 XOR하는 건 상관이 없는데
   - 스프라이트 픽셀 값이 0이고 디스플레이가 활성화 되었을 떄 
     충돌 플래그 조건이 `true`로 설정되는 버그가 있었음
   - XOR 특성을 사용하여 조기종료하거나 
     충돌 조건 확인에 스프라이트 픽셀 값이 0 인 경우를 고려했어야 함
   - 더 효율적으로 보이는 구현인 조기종료를 선택 

추가로 테스트를 위해서 `print_display()` 함수를 AI 써서 만듦.   
(이거 만들기 전에는 memory view 까서 읽었음)  
이런걸 하나하나 구현해야 하는게 C언어의 번거로운 점인듯.

## 10. 입력 기능 만들기

#### 고민 & 자료 찾기

근데 이게... 구현 방법이 명확하지 않음.

- 16개 키패드에 처리하기
- 기존 레이아웃 & 현대 컴퓨터 레이아웃까지 2개 이상의 종류를 지원하는게 좋음

> You will probably want to use keyboard scancodes rather than
> key string constants, so people who use different keyboard layouts
> (like AZERTY) can use your emulator.

그럼 입력을 어떻게 구현하는가?   
버퍼에 입력을 받고, loop마다 버퍼를 소비하는 식으로 해야 할 듯? 

그리고 `Fx0A`가 신규 키를 허용하는가 그렇지 않는가가 중요함.
- 다음 자료 참고
  - https://retrocomputing.stackexchange.com/questions/358/how-are-held-down-keys-handled-in-chip-8
- Ex9E/ExA1: 현재 키 상태를 즉시 읽어서 조건 분기. 키가 계속 눌려 있으면 계속 분기. 
- Fx0A: 키가 눌렸다가 떨어질 때까지 기다렸다가, 그 값을 저장. 이미 눌려 있던 키가 있으면 떨어질 때까지 대기.

`Fx0A`는 언제 눌렸는지는 중요하지 않고, 떨어져야 함.

그럼 이전 입력 상태를 저장하는 배열, 현재(버퍼에 있는 값이 반영된) 입력 상태를 저장하는 배열이 필요함

이 부분은 잘 모르다보니 AI를 적극적으로 활용함.
대화 내역은 [여기서](./doc/터미널%20입력%20비동기%20처리%20(2025.%205.%2013.%20오후%206：55：17).html) 확인 가능

#### 구상

대충 필요한거만 말하자면,
나는 최대한 외부 라이브러리 없이 구현하려는데, 그러려면 `termios API`가 필요함. POSIX 표준이고  
터미널 애뮬레이터의 입력 모드를 바꾸거나 출력 설정을 변경하는 기능을 제공하줌.

이를 통해서 SDL이나 외부 라이브러리 없이 터미널에 화면을 그리거나   
enter없이도 계속해서 입력을 내 프로그램으로 전달할 수 있음.

다만 이건 스캔코드 방식은 아님. 외부 라이브러리 없이 스캔코드를 하려면 
권한 설정이 필요하고 뭔가 까다로운게 많아서 안함.

구현 방법은 멀티스레드를 사용해서 계속 입력을 기록하는 스레드를 두고, 
메인 스레드에서 loop마다 그걸 확인 + 소비(비우기)하는 식으로 갈 듯.
그래서 읽을 때는 상관 없는데, 마지막에 비울 때는 뮤텍스를 거는게 좋아보임. 

키 입력을 받는 스레드에서 키 상태를 지속적으로 확인해도, 
main의 loop 단위가 아니면 타이밍 차이가 생길 수 있음.

또 스레드를 사용하지 않으면 그걸 대기하고 있어줘야 하는데, 그럼 연산에 영향을 줘서 스레드로 뺴야 함
아무리 입력을 바로 받는다고 해도, 처리 도중에 수행되는건 이상하다고 생각함.

#### 데모 코드 테스트

LLM의 도움을 받아 여러 상황에 대한 예시 코드를 제공받아 수행.

근데 테스트코드를 만들어서 해보니까 loop 단위로 하면 입력 처리가 어렵다.
일정 tick 이상은 유지해줘야 처리에 문제가 없을 거 같기도 한데, 정확하진 않음. 

왜냐면 이게 꾹 누른 상태를 인식하기가 어려울수도 있을 듯?

대부분의 OS는 터미널에서 키보드를 꾹 누르면 한 키가 계속 입력해주는데, 
이 주기가 긴 경우는 꾹 눌러도 눌렀다 뗐다 하는 것 같아 보일수도? 

타입스탬프 기반으로 n초까진 괜찮게 하거나, 
tick마다 값을 1씩 감소시키는 형태로 어느정도 프레임까지는 입력된걸로 처리하거나 하면 될 듯?

애뮬레이터니까 시간보다는 tick 단위로 처리하는게 좋아보여서 후자 선택
필요에 따라 1틱으로 처리할수도 있고...

근데 그럼 눌렀다 뗌 처리를 어떻게 하지?
이건 또 애매하네 이것도 마찬가지로 틱 기준으로 N틱 동안 갱신이 안되면 땐 상태로 봐야하나?

애매해서 명세를 자세히 보니까 새로운 입력을 받을 때까지 블로킹하고, 때었을 때 처리하는거니까
일정 시간 기다리게 해야하나? 어떻게 하지?

결론: 그냥 명세 무시하고 새로 눌렸을 때 인식하고 처리하게 하기. 나중에 필요하면 수정하면 됨.

뭐 따지고 보면 키보드 동시 입력도 안되고 부족한 부분 많으니까 이거는 일단은? 무시하기.

#### 구현

데모에 사용한 코드를 기반으로 구현 완료.

다만 main 코드가 점점 더러워지는 느낌. 이제 디스플레이랑 사운드만 하면 거의 다 만드는거긴 한데...
좀 코드 가독성 좋게 수정하고 분리할 필요성이 느껴임 함.

## 11. 미뤄둔 입력 관련 opcode 구현

`Ex9E`, `ExA1`는 그냥 구현하면 되는데, `Fx0A`가 조금 문제였다.

`Fx0A`는 눌렀다 뗌 처리가 필요한데, 이걸 하려면 조금 복잡해진다.   
불가능 한건 아니지만 일단은 새로운 키를 눌렀을 때를 기준으로 처리하도록 했다.

눌렀다가 뗌을 처리하려면 새로운 키룰 누른걸 인식하고 나서, 
INPUT_TICK이 0이 될 때까지 pc를 계속 되돌려놓으면서 완료되었다는 처리를 해줬어야 함.
그래서 새로운 키 입력 중인 상태를 기억하는 전역 변수가 필요함.

읽기 연산에서 mutex를 걸지 말지 고민했는데, 
C언어에서는 volatile 키워드를 붙여도, 멀티스레드에서 가시성(visibility)을 보장하지 않는다고 함.
그래서 mutex를 사용해야만 했음.

- [참고1](./doc/Differences%20between%20C%20and%20Java's%20volatile%20keyword%20and%20mutex%20-%20Claude%20(2025.%205.%2015.%20오후%202：17：38).html), [참고2](./doc/volatile과%20mutex%20사용%20방법%20비교.md)

테스트는 하지 않았는데, 디스플레이 구현까지 하고 다른 ROM 로드하면서 볼 것이기 때문.

## [진행 예정] 12. 터미널 기반 디스플레이 출력 구현

## [진행 예정] 13. 사운드 처리 기능 구현

# 참고자료

- https://en.wikipedia.org/wiki/CHIP-8
- https://en.wikipedia.org/wiki/Emulator
- https://tobiasvl.github.io/blog/write-a-chip-8-emulator/
- http://devernay.free.fr/hacks/chip8/C8TECH10.HTM
